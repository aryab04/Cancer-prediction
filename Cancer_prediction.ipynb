{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('The_Cancer_data_1500_V2.csv')\n",
    "\n",
    "# Drop any rows with missing values\n",
    "df = df.dropna()\n",
    "\n",
    "# Separate features (X) and target variable (y)\n",
    "X = df.drop('Outcome', axis=1).values\n",
    "y = df['Outcome'].values\n",
    "\n",
    "# Standardize features (optional but recommended)\n",
    "X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and test sets (70% training, 30% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Functions for Logistic Regression with Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def loss_function(X, y, theta, lambda_reg):\n",
    "    m = len(y)\n",
    "    h = sigmoid(X.dot(theta))\n",
    "    reg_term = (lambda_reg / (2 * m)) * np.sum(theta[1:]**2)  # regularization term\n",
    "    loss = (-1 / m) * (y.dot(np.log(h)) + (1 - y).dot(np.log(1 - h))) + reg_term\n",
    "    return loss\n",
    "\n",
    "def gradient_descent(X, y, theta, alpha, lambda_reg, num_iters):\n",
    "    m = len(y)\n",
    "    loss_history = []\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        h = sigmoid(X.dot(theta))\n",
    "        gradient = (1 / m) * X.T.dot(h - y)\n",
    "        reg_term = (lambda_reg / m) * np.concatenate(([0], theta[1:]))  # regularization term\n",
    "        theta = theta - alpha * (gradient + reg_term)\n",
    "        \n",
    "        current_loss = loss_function(X, y, theta, lambda_reg)\n",
    "        loss_history.append(current_loss)\n",
    "    \n",
    "    return theta, loss_history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Regularized Logistic Regression Model on Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add intercept term to X_train and X_test\n",
    "X_train = np.concatenate((np.ones((X_train.shape[0], 1)), X_train), axis=1)\n",
    "X_test = np.concatenate((np.ones((X_test.shape[0], 1)), X_test), axis=1)\n",
    "\n",
    "# Initialize parameters\n",
    "theta = np.zeros(X_train.shape[1])\n",
    "\n",
    "# Set hyperparameters\n",
    "alpha = 0.01\n",
    "lambda_reg = 1\n",
    "num_iters = 1000\n",
    "\n",
    "# Perform gradient descent on training data\n",
    "theta, loss_history = gradient_descent(X_train, y_train, theta, alpha, lambda_reg, num_iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Model on Test Data and Plot Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, theta):\n",
    "    h = sigmoid(X.dot(theta))\n",
    "    return (h >= 0.5).astype(int)\n",
    "\n",
    "# Calculate loss on test set\n",
    "test_loss = loss_function(X_test, y_test, theta, lambda_reg)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "\n",
    "# Plot the loss function over iterations\n",
    "plt.figure()\n",
    "plt.plot(range(1, num_iters + 1), loss_history, color='b', label='Training Loss')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss Function')\n",
    "plt.title('Loss Function vs. Iterations')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
